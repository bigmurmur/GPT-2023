# 基本配置和使用

看了吴恩达教授的视频有感而发。

## 有部分函数是我封装的函数，不是 LangChain 的内置函数（便于批量调整key和温度之类的参数）。请注意观察。下面简要列出：

```python
Get_completion(prompt = prompt)
Create_ChatModel()
CreateConversationBufferMemory(memory = memory)
```



这个包基于 $\text{python}$ ，目前包含多个功能的封装，下面将一一说明：

### GPT_BASIC

这个文件包含 $\text{GPT}$ 的基本配置。在使用此库之前请在 $\text{cmd}$ 执行以下语句（如果需要保证 $\text{key}$ 安全性加上第二条）：

```
pip install openai
pip install tiktoken
```

使用此库需要加入以下代码：

```python
from GPT_BASIC.py import Get_completion
```



使用时将需要 $\text{GPT}$ 反馈的语句传入 函数 $Get_Completion()$ 即可

注意传入语句需用 $\text{f}$ 进行字符串格式化， $\text{GPT-4}$ 则不需要。

### LangChain_BASIC

内含 $\text{LangChain}$ 诸多配置。请在使用此库之前使用 $\text{cmd}$ 执行以下语句：

````
pip install langchain
````

$\text{LangChain}$ 成功链接了不同功能，使得编写与训练更为方便。（只是一个工具，不会使用直接查用法就行了）

##### 链接 openai GPT 的抽象

链接 $\text{openai}$ 的 $\text{GPT-3.5-turbo}$ 模型，同时应学会使用 $\text{prompt}$ 模板。

可以使用这个库里的 $Create_ChatModel()$ 创建一个新的 $\text{GPT}$ 实例。

```python
from LangChain_BASIC import *
```

使用的时候直接调用以下函数，这样就省去了乱七八糟的 $API_KEY$ 之类的配置（当然也可以不用我封装的这个函数，直接 $\text{NewModel = ChatOpenAI()}$，但要配置 $\text{KEY}$）：

```python
MyModelName = Create_ChatModel()
```

调用的时候直接使用以下函数（参数都是 $\text{Langchain}$ 内置的 $\text{Prompt}$ 类）：

```python
customer_response = MyModelName(customer_message)
print(customer_response.content)
```

下面通过一个例子介绍 $\text{Prompt}$ 类的使用。基于 $\text{Langchain}$ 的 $\text{OpenGPT}$ 必须使用 $\text{Prompt}$ 类。

```python
# 创建一个 GPT 实例（吴恩达教学里面“创建实例”的意思大概是："GPT" 相当于一个物种，“创建一个 GPT 实例”相当于造了一个叫做 mygpt007 的个体，不同个体之间互不影响）
mygpt = Create_ChatModel()
# template_str 的作用相当于创造一个 prompt 模板，重复使用的时候只需要填空就可以了，不必再复制一遍整个 prompt.（例如这里只需要填空"style" 和 "text"）
template_str = """
You are a strict teacher who is going to answer your students' question.
Your words must be turn into a style that is '{style}'
Your students' question are :
{text}
"""
prompt_template = ChatPromptTemplate.from_template(template_str)

customer_style = """
The language should be American English, in a extremely angry tone.Your words should be brief.
"""
customer_email = """
Excuse me,sir. Is one plus one equal to three?
"""
#把需要填空的地方填进去就好了
customer_message = prompt_template.format_messages(style=customer_style,text=customer_email)
#向 GPT 发送信息并获取回复
customer_reponse = mygpt(customer_message)
print(customer_reponse.content)
```

####  **输出解析器** 

假如我们从 $\text{GPT}$ 那里获取了一段回复：

```
嗷，先生。我们学校有 666 个同学，他们当中有 200 个人喜欢打篮球。
```

问题来了，我们怎么让电脑把同学数量这些信息从人类的语言转化成为规范的数据呢？

$\text{LangChain}$ 为我们提供了一种可格式化的提取信息的方式，我们看一个例子就明白了：

```python
#我们创造两个概要：分别是数字和性别。
number_schema = ResponseSchema(name = "number",description = "How many students are there? Answer with the exact number.")
sex_schema = ResponseSchema(name = "sex",description = "How many boys are there? Answer with the exact number.")

#把所有的概要封装成列表。
response_schema = [number_schema,sex_schema]
#执行以下两行，生成的 format_instructions 即为提取信息的模板。直接把这个字符串塞进给 GPT 的 prompt 即可。
output_parser = StructuredOutputParser.from_response_schemas(response_schema)
format_instructions = output_parser.get_format_instructions()
#print(format_instructions)

text="""
Hey,guys! Today is our first day at school!!!! We've got 891 members here and two thirds of them are girls.
"""
# 信息来源文本：我们学校有 891 个人，其中三分之二都是女生。

template_str="""
For the following text, extract the following information:
number : How many students are there? Answer with the exact number.
sex : How many boys are there? Answer with the exact number.
text: {text}
{format_instructions}
"""
# 在 prompt 末尾加上 {format_instructions} 即可完成。接下来的过程安装普通 langchain 调用 gpt-3.5 的方式编写即可。最终的答案将会以 JSON 的格式给出。

MyGPT = Create_ChatModel()
prompt = ChatPromptTemplate.from_template(template_str)
customer_message = prompt.format_messages(text = text,format_instructions = format_instructions)

customer_response = MyGPT(customer_message)
print(customer_response.content)
```

我们发现最终结果为：

```
​```json
{
	"number": "891",
	"sex": "297"
}
​```
```

这是一个很规范的格式，便于此后的编程对于信息的提取。



#### 记忆

这个部分一共有四个主要记忆策略，它们分别为：
$$
\text{ConversationBufferMemory }\\
\text{ConversationBufferWindowMemory}\\
\text{}
$$


#####  ConversationBufferMemory 

这个函数用于临时存储。我们先创一个记忆存储系统：

```python
memory01 = ConversationBufferMemory()
```

然后我们创造一个对话：

```python
convert = CreateConversationBufferMemory(memory = memory01)
```

然后我们传入对话的时候直接传入字符串，调用方式为：

```python
convert.predict(input = "啦啦啦")
```

我们来看一个例子：

```python
mymemory = ConversationBufferMemory()
convert = CreateConversation(memory = mymemory)
convert.predict(input = "I'm 18 years old.")
convert.predict(input = "I have 8 dogs.")
convert.predict(input = "How old am I?")
convert.predict(input = "Please give me the answer : my age plus the the number of my dogs.")

print(mymemory.buffer)
```



最后输出的 $\text{memory.buffer}$ 承载的是所有的通话记录。如下：

```
> Finished chain.
Human: I'm 18 years old.
AI: That's great! Being 18 years old is an exciting time in life. You're likely finishing up high school or starting college. It's a time of new experiences and opportunities. Do you have any specific plans or goals for this stage of your life?
Human: I have 8 dogs.
AI: Wow, that's a lot of dogs! Taking care of eight dogs must be quite a handful. What are their names?
Human: How old am I?
AI: Based on the information you provided earlier, you mentioned that you are 18 years old.
Human: Please give me the answer : my age plus the the number of my dogs.
AI: Your age is 18 and you have 8 dogs, so the sum of your age and the number of your dogs is 26.

```

我们可以发现这一个 $\text{ConversationBufferMemory }$ 存储之后，记忆效果很好。

一些其他的功能有待探索。



##### ConversationBufferWindowMemory

可以限制记忆的记录条数 $k$。

使用方法类似：

```python
mymemory = ConversationBufferWindowMemory(k = 1)
convert = CreateConversation(memory = mymemory)
convert.predict(input = "I'm 18 years old.")
convert.predict(input = "I have 8 dogs.")
convert.predict(input = "How old am I?")
print(mymemory.buffer)
```

会发现输出并不记得 $18$ 岁这个历史记录。

##### ConversationSummaryBufferMemory

可以限制存储内容当中使用的最大 $\text{token}$ 数量，防止成本过大。

当超过设置的最大 $\text{token}$ 数量时，直接删除过早的聊天记录。

这一记忆方式的使用方法与上面有以下差别：

```python
chat_model = Create_ChatModel()
mymemory = ConversationTokenBufferMemory(llm = chat_model , max_token_limit = 30)
convert = ConversationChain(
    llm = chat_model, 
    memory = mymemory,
    verbose = False
)
```

剩下的使用方法类似。

##### ConversationSummaryBufferMemory

可以限制存储内容当中使用的最大 $\text{token}$ 数量，并且对前置的对话进行总结，防止成本过大。

使用方法如下：

```python
chat_model = Create_ChatModel()
mymemory = ConversationSummaryBufferMemory(llm = chat_model , max_token_limit = 30)
convert = ConversationChain(
    llm = chat_model, 
    memory = mymemory,
    verbose = False
)
```



其中最后两种记忆方式有相似之处，但请注意第四种记忆方式中 $\text{GPT}$ 写的总结不一定完备。

当然，你可以在记忆部分强制插入一些对话，例如：

```python
mymemory.save_context({"input": "Tell me 1+1 = ?"},
                    {"output": "1+1 = 3!"})
```

